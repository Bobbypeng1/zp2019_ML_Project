{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VbW7P4j_zyN",
        "outputId": "28c26ce8-ace2-498c-f284-62bbe9e7721f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu6txdbw__Nw",
        "outputId": "19e14dc1-6d41-44f6-ebfc-c4bdb4782029"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "'Bobby p3 draft.gdoc'                      zp2019_hw1.pdf\n",
            "'Bobby P3.gdoc'                            zp2019_hw2.ipynb\n",
            "'Bobby Peng Farewell .gdoc'                zp2019_hw2.pdf\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/                         zp2019_hw3.pdf\n",
            " \u001b[01;34mlazydata\u001b[0m/                                'zp2019_hw4 (1).pdf'\n",
            " \u001b[01;34mleftImg8bit_trainvaltest\u001b[0m/                'zp2019_hw4_part1 (1).py'\n",
            " NYU_SR_TUNF.pdf                           zp2019_hw4_part1.py\n",
            " NYU_Unofficial_Transcript.pdf            'zp2019_hw4_part2 (1).py'\n",
            "'P2 Draft Bobby Peng.gdoc'                 zp2019_hw4_part2.py\n",
            "'P2EX2 (1).gdoc'                           zp2019_hw4.pdf\n",
            " P2EX2.gdoc                               'zp2019_hw5 (1).ipynb'\n",
            "'PRACTICE-UA1 EXAM 1.gdoc'                'zp2019_hw5 (2).ipynb'\n",
            "'PRACTICE-UA1 EXAM 1.pdf'                 'zp2019_hw5 (3).ipynb'\n",
            "'Reflective writing Bobby Peng.gdoc'      'zp2019_hw5_extra_credit (1).ipynb'\n",
            " Resume_Bobby_Peng.pdf                    'zp2019_hw5_extra_credit (2).ipynb'\n",
            "'Screen Shot 2022-11-12 at 09.17.54.png'   zp2019_hw5_extra_credit.ipynb\n",
            "'zp2019_extracredit (1).out'               zp2019_hw5.ipynb\n",
            "'zp2019_extracredit (1).py'                zp2019_hw6.ipynb\n",
            " zp2019_extracredit.out                    大哥结婚.PNG\n",
            " zp2019_extracredit.py                     截屏2020-12-16下午11.47.19.png\n",
            " zp2019_hw1.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle as pkl\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "9RM7RI3FALFk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LazyLoadDataset(Dataset):\n",
        "  def __init__(self, path, train=True, transform=None):\n",
        "    self.transform = transform\n",
        "    path = path + ('train/' if train else 'test/')\n",
        "    self.pathX = path + 'X/'\n",
        "    self.pathY = path + 'Y/'\n",
        "    self.data = os.listdir(self.pathX)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    f = self.data[idx]\n",
        "    img0 = cv2.imread(self.pathX+f+'/rgb/0.png')\n",
        "    img1 = cv2.imread(self.pathX+f+'/rgb/1.png')\n",
        "    img2 = cv2.imread(self.pathX+f+'/rgb/2.png')\n",
        "    if self.transform is not None:\n",
        "      img0 = self.transform(img0)\n",
        "      img1 = self.transform(img1)\n",
        "      img2 = self.transform(img2)\n",
        "    depth = np.load(self.pathX+f+'/depth.npy')\n",
        "    field_id = pkl.load(open(self.pathX+f+'/field_id.pkl', 'rb'))\n",
        "    Y = np.load(self.pathY+f+'.npy')\n",
        "    return (img0, img1, img2, depth, field_id), Y\n",
        "  \n",
        "  def getTestitem(self, idx):\n",
        "    f = self.data[idx]\n",
        "    img0 = cv2.imread(self.pathX+f+'/rgb/0.png')\n",
        "    img1 = cv2.imread(self.pathX+f+'/rgb/1.png')\n",
        "    img2 = cv2.imread(self.pathX+f+'/rgb/2.png')\n",
        "    if self.transform is not None:\n",
        "      img0 = self.transform(img0)\n",
        "      img1 = self.transform(img1)\n",
        "      img2 = self.transform(img2)\n",
        "    depth = np.load(self.pathX+f+'/depth.npy')\n",
        "    field_id = pkl.load(open(self.pathX+f+'/field_id.pkl', 'rb'))\n",
        "    return img0, img1, img2, depth, field_id\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "kuPd2Ng1ANrY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find mean and std\n",
        "'''\n",
        "train_data = LazyLoadDataset('./lazydata/', transform=transforms.Compose([transforms.ToTensor()]))\n",
        "train_loader = DataLoader(train_data, batch_size=12, shuffle=True)\n",
        "train_it = iter(train_loader)\n",
        "def get_mean_std(loader):\n",
        "  c=0\n",
        "  mean0, mean1, mean2, std0, std1, std2, total_image_count= 0, 0, 0, 0, 0, 0, 0\n",
        "  now = next(train_it)\n",
        "  while True:\n",
        "    (img0, img1, img2, depth, field_id), Y = now\n",
        "    image_count_in_batch= img0.size(0)\n",
        "    img0 = img0.view(image_count_in_batch, img0.size(1), -1)\n",
        "    img1 = img1.view(image_count_in_batch, img1.size(1), -1)\n",
        "    img2 = img2.view(image_count_in_batch, img2.size(1), -1)\n",
        "    mean0 += img0.mean(2).sum(0)\n",
        "    mean1 += img1.mean(2).sum(0)\n",
        "    mean2 += img2.mean(2).sum(0)\n",
        "    std0 += img0.std(2).sum(0)\n",
        "    std1 += img1.std(2).sum(0)\n",
        "    std2 += img2.std(2).sum(0)\n",
        "    total_image_count += image_count_in_batch\n",
        "    print(c)\n",
        "    if c == 282: break\n",
        "    c+=1\n",
        "    now = next(train_it)\n",
        "\n",
        "  mean0 /= total_image_count\n",
        "  mean1 /= total_image_count\n",
        "  mean2 /= total_image_count\n",
        "  std0 /= total_image_count\n",
        "  std1 /= total_image_count\n",
        "  std2 /= total_image_count\n",
        "  return mean0, mean1, mean2, std0, std1, std2\n",
        "\n",
        "mean0, mean1, mean2, std0, std1, std2 = get_mean_std(train_loader)\n",
        "total_mean = (mean0+mean1+mean2)/3\n",
        "total_std = (std0+std1+std2)/3\n",
        "print(total_mean, total_std)\n",
        "print(mean0, mean1, mean2, std0, std1, std2)\n",
        "'''\n",
        "#tensor([0.4851, 0.4623, 0.4356]) tensor([0.2187, 0.2176, 0.2331])\n",
        "\n",
        "#tensor([0.4352, 0.4170, 0.3960]) tensor([0.5008, 0.4879, 0.4697]) tensor([0.5193, 0.4820, 0.4412]) \n",
        "#tensor([0.1992, 0.1987, 0.2111]) tensor([0.2276, 0.2252, 0.2417]) tensor([0.2293, 0.2288, 0.2465])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "CHvkYkVpAVYs",
        "outputId": "f982f2b4-117f-431e-a6a0-e031a8a680dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrain_data = LazyLoadDataset('./lazydata/', transform=transforms.Compose([transforms.ToTensor()]))\\ntrain_loader = DataLoader(train_data, batch_size=12, shuffle=True)\\ntrain_it = iter(train_loader)\\ndef get_mean_std(loader):\\n  c=0\\n  mean0, mean1, mean2, std0, std1, std2, total_image_count= 0, 0, 0, 0, 0, 0, 0\\n  now = next(train_it)\\n  while True:\\n    (img0, img1, img2, depth, field_id), Y = now\\n    image_count_in_batch= img0.size(0)\\n    img0 = img0.view(image_count_in_batch, img0.size(1), -1)\\n    img1 = img1.view(image_count_in_batch, img1.size(1), -1)\\n    img2 = img2.view(image_count_in_batch, img2.size(1), -1)\\n    mean0 += img0.mean(2).sum(0)\\n    mean1 += img1.mean(2).sum(0)\\n    mean2 += img2.mean(2).sum(0)\\n    std0 += img0.std(2).sum(0)\\n    std1 += img1.std(2).sum(0)\\n    std2 += img2.std(2).sum(0)\\n    total_image_count += image_count_in_batch\\n    print(c)\\n    if c == 282: break\\n    c+=1\\n    now = next(train_it)\\n\\n  mean0 /= total_image_count\\n  mean1 /= total_image_count\\n  mean2 /= total_image_count\\n  std0 /= total_image_count\\n  std1 /= total_image_count\\n  std2 /= total_image_count\\n  return mean0, mean1, mean2, std0, std1, std2\\n\\nmean0, mean1, mean2, std0, std1, std2 = get_mean_std(train_loader)\\ntotal_mean = (mean0+mean1+mean2)/3\\ntotal_std = (std0+std1+std2)/3\\nprint(total_mean, total_std)\\nprint(mean0, mean1, mean2, std0, std1, std2)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define transform(totensor and normalize)\n",
        "mean, std = [0.4851, 0.4623, 0.4356], [0.2187, 0.2176, 0.2331]\n",
        "my_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))])"
      ],
      "metadata": {
        "id": "4OAME8-sIXRC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = LazyLoadDataset('./lazydata/', transform=my_transform)\n",
        "test_data = LazyLoadDataset('./lazydata/', train=False, transform=my_transform)"
      ],
      "metadata": {
        "id": "ItHrgHkaIk5c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=12, shuffle=True)\n",
        "train_it = iter(train_loader)"
      ],
      "metadata": {
        "id": "FcOVZhpqI-bW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TKlk911DJk-6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, ((img0, img1, img2, depth, field_id), Y) in enumerate(train_loader):\n",
        "        for batch in range(12):\n",
        "          i0, i1, i2, d, id, y = img0[batch], img1[batch], img2[batch], depth[batch], field_id[batch], Y[batch]\n",
        "          data = torch.stack((i0, i1, i2), 0)\n",
        "          data =data.view(9, 224, -1)\n",
        "          y = y.view(1, -1)\n",
        "          torch.mul(y, 1000)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          output = model(data)\n",
        "          torch.mul(output, 1000)\n",
        "          lossfn = nn.MSELoss()\n",
        "          loss = torch.sqrt(lossfn(output.float(), y.float()))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * 12, len(train_data),\n",
        "                100. * batch_idx * 12 / len(train_data), loss.item()))                                                  "
      ],
      "metadata": {
        "id": "_MY649tKJq1U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=5)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
        "        self.LF1 = nn.Linear(conv_feature*49*49, fc_feature)\n",
        "        self.LF2 = nn.Linear(fc_feature, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.maxpool1(F.relu(self.conv1(x)))\n",
        "        x = self.maxpool2(F.relu(self.conv2(x)))\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 6*49*49)\n",
        "        x = F.relu(self.LF1(x))\n",
        "        x = self.LF2(x)\n",
        "        return x\n",
        "\n",
        "    #score 0.0073"
      ],
      "metadata": {
        "id": "DwcFcEmQMTGh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings \n",
        "conv_features = 6 # number of feature maps\n",
        "fc_features = 50\n",
        "output_size = 12\n",
        "\n",
        "model_cnn = CNN(9, conv_features, fc_features, output_size) # create CNN model\n",
        "model_cnn.to(device)\n",
        "optimizer = torch.optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "tvYoTdUoMbcV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 1):\n",
        "    train(epoch, model_cnn, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV9sPrZ8Memw",
        "outputId": "8fb2e058-0c02-4f93-a623-931cb8919437"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/3396 (0%)]\tLoss: 0.034368\n",
            "Train Epoch: 0 [120/3396 (4%)]\tLoss: 0.015445\n",
            "Train Epoch: 0 [240/3396 (7%)]\tLoss: 0.013365\n",
            "Train Epoch: 0 [360/3396 (11%)]\tLoss: 0.016154\n",
            "Train Epoch: 0 [480/3396 (14%)]\tLoss: 0.015894\n",
            "Train Epoch: 0 [600/3396 (18%)]\tLoss: 0.016623\n",
            "Train Epoch: 0 [720/3396 (21%)]\tLoss: 0.012801\n",
            "Train Epoch: 0 [840/3396 (25%)]\tLoss: 0.011902\n",
            "Train Epoch: 0 [960/3396 (28%)]\tLoss: 0.010370\n",
            "Train Epoch: 0 [1080/3396 (32%)]\tLoss: 0.010873\n",
            "Train Epoch: 0 [1200/3396 (35%)]\tLoss: 0.007087\n",
            "Train Epoch: 0 [1320/3396 (39%)]\tLoss: 0.008341\n",
            "Train Epoch: 0 [1440/3396 (42%)]\tLoss: 0.007633\n",
            "Train Epoch: 0 [1560/3396 (46%)]\tLoss: 0.009321\n",
            "Train Epoch: 0 [1680/3396 (49%)]\tLoss: 0.004584\n",
            "Train Epoch: 0 [1800/3396 (53%)]\tLoss: 0.010414\n",
            "Train Epoch: 0 [1920/3396 (57%)]\tLoss: 0.013201\n",
            "Train Epoch: 0 [2040/3396 (60%)]\tLoss: 0.005483\n",
            "Train Epoch: 0 [2160/3396 (64%)]\tLoss: 0.005895\n",
            "Train Epoch: 0 [2280/3396 (67%)]\tLoss: 0.004108\n",
            "Train Epoch: 0 [2400/3396 (71%)]\tLoss: 0.005704\n",
            "Train Epoch: 0 [2520/3396 (74%)]\tLoss: 0.005890\n",
            "Train Epoch: 0 [2640/3396 (78%)]\tLoss: 0.009451\n",
            "Train Epoch: 0 [2760/3396 (81%)]\tLoss: 0.005979\n",
            "Train Epoch: 0 [2880/3396 (85%)]\tLoss: 0.003540\n",
            "Train Epoch: 0 [3000/3396 (88%)]\tLoss: 0.004561\n",
            "Train Epoch: 0 [3120/3396 (92%)]\tLoss: 0.004422\n",
            "Train Epoch: 0 [3240/3396 (95%)]\tLoss: 0.005171\n",
            "Train Epoch: 0 [3360/3396 (99%)]\tLoss: 0.004790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings \n",
        "conv_features = 6 # number of feature maps\n",
        "fc_features = 25\n",
        "output_size = 12\n",
        "\n",
        "model_cnn = CNN(9, conv_features, fc_features, output_size) # create CNN model\n",
        "model_cnn.to(device)\n",
        "optimizer = torch.optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
        "for epoch in range(0, 1):\n",
        "    train(epoch, model_cnn, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCs7RSZKOMu2",
        "outputId": "24a7b04b-92bc-4be8-93d3-62fa1043e62e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/3396 (0%)]\tLoss: 0.043899\n",
            "Train Epoch: 0 [120/3396 (4%)]\tLoss: 0.034656\n",
            "Train Epoch: 0 [240/3396 (7%)]\tLoss: 0.015240\n",
            "Train Epoch: 0 [360/3396 (11%)]\tLoss: 0.017372\n",
            "Train Epoch: 0 [480/3396 (14%)]\tLoss: 0.012820\n",
            "Train Epoch: 0 [600/3396 (18%)]\tLoss: 0.016436\n",
            "Train Epoch: 0 [720/3396 (21%)]\tLoss: 0.018251\n",
            "Train Epoch: 0 [840/3396 (25%)]\tLoss: 0.012948\n",
            "Train Epoch: 0 [960/3396 (28%)]\tLoss: 0.011732\n",
            "Train Epoch: 0 [1080/3396 (32%)]\tLoss: 0.013824\n",
            "Train Epoch: 0 [1200/3396 (35%)]\tLoss: 0.015254\n",
            "Train Epoch: 0 [1320/3396 (39%)]\tLoss: 0.009157\n",
            "Train Epoch: 0 [1440/3396 (42%)]\tLoss: 0.008217\n",
            "Train Epoch: 0 [1560/3396 (46%)]\tLoss: 0.006940\n",
            "Train Epoch: 0 [1680/3396 (49%)]\tLoss: 0.007246\n",
            "Train Epoch: 0 [1800/3396 (53%)]\tLoss: 0.010749\n",
            "Train Epoch: 0 [1920/3396 (57%)]\tLoss: 0.004687\n",
            "Train Epoch: 0 [2040/3396 (60%)]\tLoss: 0.010433\n",
            "Train Epoch: 0 [2160/3396 (64%)]\tLoss: 0.007694\n",
            "Train Epoch: 0 [2280/3396 (67%)]\tLoss: 0.009651\n",
            "Train Epoch: 0 [2400/3396 (71%)]\tLoss: 0.005014\n",
            "Train Epoch: 0 [2520/3396 (74%)]\tLoss: 0.010462\n",
            "Train Epoch: 0 [2640/3396 (78%)]\tLoss: 0.006972\n",
            "Train Epoch: 0 [2760/3396 (81%)]\tLoss: 0.007166\n",
            "Train Epoch: 0 [2880/3396 (85%)]\tLoss: 0.010990\n",
            "Train Epoch: 0 [3000/3396 (88%)]\tLoss: 0.005417\n",
            "Train Epoch: 0 [3120/3396 (92%)]\tLoss: 0.007241\n",
            "Train Epoch: 0 [3240/3396 (95%)]\tLoss: 0.006665\n",
            "Train Epoch: 0 [3360/3396 (99%)]\tLoss: 0.005290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN2(nn.Module):\n",
        "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=5)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(conv_feature, conv_feature, kernel_size=3)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv4 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
        "        self.LF1 = nn.Linear(conv_feature*21*21, fc_feature)\n",
        "        self.LF2 = nn.Linear(fc_feature, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.maxpool1(F.relu(self.conv1(x)))\n",
        "        x = self.maxpool2(F.relu(self.conv2(x)))\n",
        "        x = self.maxpool3(F.relu(self.conv3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = x.view(-1, 6*21*21)\n",
        "        x = F.relu(self.LF1(x))\n",
        "        x = self.LF2(x)\n",
        "        return x\n",
        "\n",
        "    #score 0.0073"
      ],
      "metadata": {
        "id": "1vmVuZ2ic3Sz"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings \n",
        "conv_features = 6 # number of feature maps\n",
        "fc_features = 50\n",
        "output_size = 12\n",
        "\n",
        "model_cnn2 = CNN2(9, conv_features, fc_features, output_size) # create CNN model\n",
        "model_cnn2.to(device)\n",
        "optimizer = torch.optim.SGD(model_cnn2.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "Mb2qzDCldiCb"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 1):\n",
        "    train(epoch, model_cnn2, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-WCQd1fGmE",
        "outputId": "b10b961c-5d2b-49f1-9d62-230e0bfa53ce"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/3396 (0%)]\tLoss: 0.065336\n",
            "Train Epoch: 0 [120/3396 (4%)]\tLoss: 0.013173\n",
            "Train Epoch: 0 [240/3396 (7%)]\tLoss: 0.026738\n",
            "Train Epoch: 0 [360/3396 (11%)]\tLoss: 0.016703\n",
            "Train Epoch: 0 [480/3396 (14%)]\tLoss: 0.018942\n",
            "Train Epoch: 0 [600/3396 (18%)]\tLoss: 0.018432\n",
            "Train Epoch: 0 [720/3396 (21%)]\tLoss: 0.017284\n",
            "Train Epoch: 0 [840/3396 (25%)]\tLoss: 0.011171\n",
            "Train Epoch: 0 [960/3396 (28%)]\tLoss: 0.007906\n",
            "Train Epoch: 0 [1080/3396 (32%)]\tLoss: 0.021231\n",
            "Train Epoch: 0 [1200/3396 (35%)]\tLoss: 0.013534\n",
            "Train Epoch: 0 [1320/3396 (39%)]\tLoss: 0.008936\n",
            "Train Epoch: 0 [1440/3396 (42%)]\tLoss: 0.016263\n",
            "Train Epoch: 0 [1560/3396 (46%)]\tLoss: 0.011361\n",
            "Train Epoch: 0 [1680/3396 (49%)]\tLoss: 0.006430\n",
            "Train Epoch: 0 [1800/3396 (53%)]\tLoss: 0.006261\n",
            "Train Epoch: 0 [1920/3396 (57%)]\tLoss: 0.005378\n",
            "Train Epoch: 0 [2040/3396 (60%)]\tLoss: 0.008962\n",
            "Train Epoch: 0 [2160/3396 (64%)]\tLoss: 0.008382\n",
            "Train Epoch: 0 [2280/3396 (67%)]\tLoss: 0.007077\n",
            "Train Epoch: 0 [2400/3396 (71%)]\tLoss: 0.013980\n",
            "Train Epoch: 0 [2520/3396 (74%)]\tLoss: 0.011967\n",
            "Train Epoch: 0 [2640/3396 (78%)]\tLoss: 0.011721\n",
            "Train Epoch: 0 [2760/3396 (81%)]\tLoss: 0.006556\n",
            "Train Epoch: 0 [2880/3396 (85%)]\tLoss: 0.016052\n",
            "Train Epoch: 0 [3000/3396 (88%)]\tLoss: 0.010175\n",
            "Train Epoch: 0 [3120/3396 (92%)]\tLoss: 0.006215\n",
            "Train Epoch: 0 [3240/3396 (95%)]\tLoss: 0.013934\n",
            "Train Epoch: 0 [3360/3396 (99%)]\tLoss: 0.009355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN3(nn.Module):\n",
        "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
        "        super(CNN3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=9)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(conv_feature, conv_feature, kernel_size=9)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv2d(conv_feature, conv_feature, kernel_size=9)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv4 = nn.Conv2d(conv_feature, conv_feature, kernel_size=9)\n",
        "        self.LF1 = nn.Linear(conv_feature*13*13, fc_feature)\n",
        "        self.LF2 = nn.Linear(fc_feature, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.maxpool1(F.relu(self.conv1(x)))\n",
        "        x = self.maxpool2(F.relu(self.conv2(x)))\n",
        "        x = self.maxpool3(F.relu(self.conv3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = x.view(-1, 6*13*13)\n",
        "        x = F.relu(self.LF1(x))\n",
        "        x = self.LF2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wZiS5TbEijUE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings \n",
        "conv_features = 6 # number of feature maps\n",
        "fc_features = 50\n",
        "output_size = 12\n",
        "\n",
        "model_cnn3 = CNN3(9, conv_features, fc_features, output_size) # create CNN model\n",
        "model_cnn3.to(device)\n",
        "optimizer = torch.optim.SGD(model_cnn3.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "gll_G0Iij2JC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 1):\n",
        "    train(epoch, model_cnn3, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVhQopiLj7XQ",
        "outputId": "3601b6bd-95c6-4b19-81a4-408b5f995064"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/3396 (0%)]\tLoss: 0.059391\n",
            "Train Epoch: 0 [120/3396 (4%)]\tLoss: 0.016135\n",
            "Train Epoch: 0 [240/3396 (7%)]\tLoss: 0.019092\n",
            "Train Epoch: 0 [360/3396 (11%)]\tLoss: 0.018479\n",
            "Train Epoch: 0 [480/3396 (14%)]\tLoss: 0.014485\n",
            "Train Epoch: 0 [600/3396 (18%)]\tLoss: 0.015617\n",
            "Train Epoch: 0 [720/3396 (21%)]\tLoss: 0.013822\n",
            "Train Epoch: 0 [840/3396 (25%)]\tLoss: 0.012890\n",
            "Train Epoch: 0 [960/3396 (28%)]\tLoss: 0.011017\n",
            "Train Epoch: 0 [1080/3396 (32%)]\tLoss: 0.010539\n",
            "Train Epoch: 0 [1200/3396 (35%)]\tLoss: 0.011207\n",
            "Train Epoch: 0 [1320/3396 (39%)]\tLoss: 0.015509\n",
            "Train Epoch: 0 [1440/3396 (42%)]\tLoss: 0.026937\n",
            "Train Epoch: 0 [1560/3396 (46%)]\tLoss: 0.018449\n",
            "Train Epoch: 0 [1680/3396 (49%)]\tLoss: 0.009970\n",
            "Train Epoch: 0 [1800/3396 (53%)]\tLoss: 0.023841\n",
            "Train Epoch: 0 [1920/3396 (57%)]\tLoss: 0.008393\n",
            "Train Epoch: 0 [2040/3396 (60%)]\tLoss: 0.019636\n",
            "Train Epoch: 0 [2160/3396 (64%)]\tLoss: 0.013843\n",
            "Train Epoch: 0 [2280/3396 (67%)]\tLoss: 0.012460\n",
            "Train Epoch: 0 [2400/3396 (71%)]\tLoss: 0.006484\n",
            "Train Epoch: 0 [2520/3396 (74%)]\tLoss: 0.012949\n",
            "Train Epoch: 0 [2640/3396 (78%)]\tLoss: 0.009345\n",
            "Train Epoch: 0 [2760/3396 (81%)]\tLoss: 0.005139\n",
            "Train Epoch: 0 [2880/3396 (85%)]\tLoss: 0.011844\n",
            "Train Epoch: 0 [3000/3396 (88%)]\tLoss: 0.008661\n",
            "Train Epoch: 0 [3120/3396 (92%)]\tLoss: 0.010381\n",
            "Train Epoch: 0 [3240/3396 (95%)]\tLoss: 0.008166\n",
            "Train Epoch: 0 [3360/3396 (99%)]\tLoss: 0.005600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN4(nn.Module):\n",
        "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
        "        super(CNN3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=9)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(conv_feature, 12, kernel_size=9)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv2d(12, 24, kernel_size=9)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv4 = nn.Conv2d(24, 48, kernel_size=9)\n",
        "        self.LF1 = nn.Linear(48*13*13, fc_feature)\n",
        "        self.LF2 = nn.Linear(fc_feature, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.maxpool1(F.relu(self.conv1(x)))\n",
        "        x = self.maxpool2(F.relu(self.conv2(x)))\n",
        "        x = self.maxpool3(F.relu(self.conv3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = x.view(-1, 48*13*13)\n",
        "        x = F.relu(self.LF1(x))\n",
        "        x = self.LF2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rVVb7_gP9vPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings \n",
        "conv_features = 6 # number of feature maps\n",
        "fc_features = 50\n",
        "output_size = 12\n",
        "\n",
        "model_cnn4 = CNN3(9, conv_features, fc_features, output_size) # create CNN model\n",
        "model_cnn4.to(device)\n",
        "optimizer = torch.optim.SGD(model_cnn4.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "yGgLsYb297aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "outfile = 'submission.csv'\n",
        "\n",
        "titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
        "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
        "preds = []\n",
        "\n",
        "\n",
        "model_cnn4.eval()\n",
        "with open(outfile, 'w') as csvfile: \n",
        "    # creating a csv writer object \n",
        "    csvwriter = csv.writer(csvfile) \n",
        "        \n",
        "    # writing the fields \n",
        "    csvwriter.writerow(titles)\n",
        "    for i in range(len(test_data)):\n",
        "      if i % 10 == 0: print(i)\n",
        "      img0, img1, img2, depth, field_id = test_data.getTestitem(i)\n",
        "      data = torch.stack((img0, img1, img2), 0)\n",
        "      data =data.view(9, 224, -1)\n",
        "      output = model_cnn4(data.float())  \n",
        "      output = output.tolist()\n",
        "      output = output[0]\n",
        "      output.insert(0, field_id)\n",
        "      csvwriter.writerow(output)\n",
        "print(\"Written to csv file {}\".format(outfile))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yoa1H6LQSnwm",
        "outputId": "b73d1d02-dd1d-4464-b20b-03c0151a96a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "440\n",
            "450\n",
            "460\n",
            "470\n",
            "480\n",
            "490\n",
            "500\n",
            "510\n",
            "520\n",
            "530\n",
            "540\n",
            "550\n",
            "560\n",
            "570\n",
            "580\n",
            "590\n",
            "600\n",
            "610\n",
            "620\n",
            "630\n",
            "640\n",
            "650\n",
            "660\n",
            "670\n",
            "680\n",
            "690\n",
            "700\n",
            "710\n",
            "720\n",
            "730\n",
            "740\n",
            "750\n",
            "760\n",
            "770\n",
            "780\n",
            "790\n",
            "800\n",
            "810\n",
            "820\n",
            "830\n",
            "840\n",
            "Written to csv file submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dipr7XV5SXz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}